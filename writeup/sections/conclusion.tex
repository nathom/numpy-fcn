\section{Conclusion}

In conclusion, our exploration of handwritten digit classification involved the utilization of the MNIST dataset and various methods, including data splitting, normalization, softmax regression, backpropagation, momentum, early stopping, and regularization. We observed that different techniques impacted the model's performance, with trade-offs between convergence speed and accuracy. The choice of activation functions, such as tanh, sigmoid, and ReLU, also played a crucial role. Our findings contribute to understanding the intricacies of training neural networks for digit classification, offering insights into optimization strategies and their effects on model performance.

Despite achieving $~98\%$ accuracy on the testing set, our best models were still left
confused by many digits, some of which we display below \cref{fig:highest_loss_digits}.
