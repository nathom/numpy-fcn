\begin{abstract}
	The paper explores backpropagation, employing numerical approximation to verify accuracy. It introduces momentum in stochastic gradient descent, demonstrating its impact on model convergence and the trade-off with slight performance penalty. The study delves into regularization experiments (L1, L2) and observes the trade-off between convergence speed and accuracy. Activation function experiments (tanh, sigmoid, ReLU) highlight their impact on test accuracy and convergence speed, with tanh outperforming others in terms of both accuracy and speed. Adjusting learning rates addresses issues with ReLU's initial high rate. We also display the most difficult to classify digits. Overall, the findings contribute insights into optimizing neural networks.
	% This paper explores various methods for handwritten digit classification using the MNIST dataset. The dataset is split, normalized, and subjected to softmax regression, backpropagation, momentum, early stopping, and regularization techniques. Activation functions (tanh, sigmoid, ReLU) are evaluated. The study reveals trade-offs between convergence speed and accuracy, contributing insights into neural network training for digit classification.
\end{abstract}
