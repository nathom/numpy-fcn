# Running with epochs = 100, batch_size = 128, patience_limit = 3, gamma = 0.7
# T: 99.99%, V: 97.50%, P: 2:  14%|█████▋                                   | 14/100 [00:12<01:12,  1.19epoch/s]
# Early stopping. Bad performance for more than 3 consecutive epochs.
# Test Accuracy: 0.9777  Test Loss: 0.09061645002621829

layer_specs: [784, 128, 10]

# Type of non-linear activation function to be used for the layers.
activation: "ReLU"

# The learning rate to be used for training.
learning_rate: 0.001

# Number of training samples per batch to be passed to network
batch_size: 128

# Number of epochs to train the model
epochs: 100

# Flag to enable early stopping
early_stop: True

# History for early stopping. Wait for this many epochs to check validation loss / accuracy.
early_stop_epoch: 3

# Regularization constant
L2_penalty: 0

# Use momentum for training
momentum: True

# Value for the parameter 'gamma' in momentum
momentum_gamma: 0.7
